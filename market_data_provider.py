import os
import time
import threading
import logging
import random
from collections import defaultdict
from typing import Dict, Any, Optional, Tuple, NamedTuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# ğŸ”¥ åŒé€šé“Kçº¿æ•°æ®æ”¯æŒ
try:
    from dual_channel_ohlcv import (
        DualChannelOHLCV, 
        IncrementalFetcher, 
        get_incremental_fetcher,
        get_timeframe_ms,
        InsufficientDataError
    )
    DUAL_CHANNEL_AVAILABLE = True
except ImportError:
    DUAL_CHANNEL_AVAILABLE = False
    logger.warning("dual_channel_ohlcv module not available, dual channel features disabled")


# ============ ğŸ”¥ æ™ºèƒ½Kçº¿ç¼“å­˜æ•°æ®ç»“æ„ ============
@dataclass
class OHLCVCacheEntry:
    """Kçº¿ç¼“å­˜æ¡ç›® - åŸºäºæ—¶é—´æˆ³å¢é•¿çš„æ™ºèƒ½ç¼“å­˜"""
    data: list                    # Kçº¿æ•°æ® [[ts, o, h, l, c, v], ...]
    last_max_ts: int              # data ä¸­æœ€å¤§çš„æ—¶é—´æˆ³ (ms)
    fetched_at_ms: int            # æ‹‰å–æ—¶åˆ»çš„ UTC ms
    is_stale: bool = False        # æ˜¯å¦åˆ¤å®šä¸ºé™ˆæ—§ï¼ˆäº¤æ˜“æ‰€æœªæ›´æ–°ï¼‰
    stale_count: int = 0          # è¿ç»­é™ˆæ—§æ¬¡æ•°
    bars_count: int = 0           # Kçº¿æ•°é‡
    is_initialized: bool = False  # æ˜¯å¦å·²å®Œæˆå…¨é‡åˆå§‹åŒ–ï¼ˆ1000æ ¹ï¼‰


# ============ ğŸ”¥ ç¼“å­˜é…ç½®å¸¸é‡ ============
OHLCV_TARGET_BARS = 1000          # ç›®æ ‡Kçº¿æ•°é‡
OHLCV_INCREMENTAL_LIMIT = 50      # å¢é‡æ‹‰å–æ•°é‡
OHLCV_PAGE_SIZE = 100             # OKX å•æ¬¡è¿”å›ä¸Šé™ï¼ˆä¿å®ˆå€¼ï¼‰
OHLCV_MAX_PAGES = 15              # æœ€å¤§åˆ†é¡µæ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰


def _get_timeframe_ms(timeframe: str) -> int:
    """è·å–æ—¶é—´å‘¨æœŸå¯¹åº”çš„æ¯«ç§’æ•°"""
    if DUAL_CHANNEL_AVAILABLE:
        return get_timeframe_ms(timeframe)
    
    # å¤‡ç”¨å®ç°
    tf_map = {
        '1m': 60 * 1000,
        '3m': 3 * 60 * 1000,
        '5m': 5 * 60 * 1000,
        '15m': 15 * 60 * 1000,
        '30m': 30 * 60 * 1000,
        '1h': 60 * 60 * 1000,
        '4h': 4 * 60 * 60 * 1000,
        '1d': 24 * 60 * 60 * 1000,
    }
    return tf_map.get(timeframe, 60 * 1000)

class MarketDataProvider:
    def __init__(self, exchange_adapter, timeframe, ohlcv_limit,
                 ohlcv_ttl_sec=None, ticker_ttl_sec=None,
                 balance_ttl_sec=None, positions_ttl_sec=None):
        # äº¤æ˜“æ‰€é€‚é…å™¨
        self.exchange = exchange_adapter
        
        # é»˜è®¤å‚æ•°
        self.timeframe = timeframe
        self.ohlcv_limit = ohlcv_limit
        
        # TTLé…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæœ‰é»˜è®¤å€¼
        self.OHLCV_TTL_SEC = int(os.getenv("OHLCV_TTL_SEC", ohlcv_ttl_sec or "30"))
        self.TICKER_TTL_SEC = int(os.getenv("TICKER_TTL_SEC", ticker_ttl_sec or "10"))
        self.BALANCE_TTL_SEC = int(os.getenv("BALANCE_TTL_SEC", balance_ttl_sec or "60"))
        self.POSITIONS_TTL_SEC = int(os.getenv("POSITIONS_TTL_SEC", positions_ttl_sec or "30"))
        
        # ç¼“å­˜å­˜å‚¨
        self.ohlcv_cache = {}
        self.ticker_cache = {}
        self.balance_cache = {}
        self.positions_cache = {}
        
        # å•èˆªç­å»é‡é”å’Œäº‹ä»¶
        self.locks = defaultdict(threading.Lock)
        self.pending = defaultdict(threading.Event)
        
        # æŒ‡æ ‡è®°å½•
        self.metrics = {
            "api_calls": 0,
            "api_latency_ms": [],
            "cache_hits": 0,
            "cache_misses": 0,
            "errors": 0,
            "last_error_time": 0
        }
        
        # ç†”æ–­çŠ¶æ€
        self.circuit_breakers = defaultdict(dict)
        # é”™è¯¯èŠ‚æµ
        self.last_error_summary = 0
        self.error_counts = defaultdict(int)
        
        # ğŸ”¥ å¾…åˆå§‹åŒ–é˜Ÿåˆ—ï¼šè®°å½•é¦–æ¬¡æ‹‰å–å¤±è´¥çš„å¸ç§ï¼Œä¸‹ä¸€è½®ä¼˜å…ˆé‡è¯•
        self.pending_init: Dict[Tuple[str, str], int] = {}  # {(symbol, tf): retry_count}
    
    def _request_with_retry(self, endpoint, symbol, func, *args, **kwargs):
        """
        å¸¦é‡è¯•çš„è¯·æ±‚æ‰§è¡Œå‡½æ•°
        
        å‚æ•°:
        - endpoint: ç«¯ç‚¹åç§°
        - symbol: äº¤æ˜“å¯¹
        - func: è¦æ‰§è¡Œçš„å‡½æ•°
        - args: ä½ç½®å‚æ•°
        - kwargs: å…³é”®å­—å‚æ•°
        
        è¿”å›:
        - å‡½æ•°æ‰§è¡Œç»“æœ
        """
        max_retries = 3
        base_delay = 0.2  # åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        max_delay = 1.0   # æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        
        for retry in range(max_retries):
            try:
                start_time = time.time()
                result = func(*args, **kwargs)
                api_latency = (time.time() - start_time) * 1000
                
                # æ›´æ–°æŒ‡æ ‡
                self.metrics["api_calls"] += 1
                self.metrics["api_latency_ms"].append(api_latency)
                
                return result, api_latency
            except Exception as e:
                # è®°å½•é”™è¯¯
                self.metrics["errors"] += 1
                self.record_error(endpoint, symbol, str(e))
                
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡é‡è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                if retry == max_retries - 1:
                    raise
                
                # è®¡ç®—æŒ‡æ•°é€€é¿æ—¶é—´ + æŠ–åŠ¨
                delay = base_delay * (2 ** retry)
                # æ·»åŠ æŠ–åŠ¨ (0.5-1.5å€çš„è®¡ç®—å»¶è¿Ÿ)
                jitter = delay * (0.5 + random.random())
                # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§å»¶è¿Ÿ
                final_delay = min(jitter, max_delay)
                
                logger.warning(f"[é‡è¯•] {endpoint} {symbol} - {e}ï¼Œå°†åœ¨ {final_delay:.2f}s åé‡è¯•")
                time.sleep(final_delay)
        
        # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œï¼Œä½†ä¸ºäº†ç±»å‹å®‰å…¨
        raise Exception("é‡è¯•æ¬¡æ•°è€—å°½")

    def get_ohlcv(self, symbol, timeframe=None, limit=None, force_fetch=False) -> Tuple[list, bool]:
        """
        è·å–Kçº¿æ•°æ® - é¦–æ¬¡å…¨é‡åˆ†é¡µæ‹‰å– + åç»­è½»é‡å¢é‡æ›´æ–° + ç¼“å­˜å›ºå®šé•¿åº¦
        
        å‚æ•°:
        - symbol: äº¤æ˜“å¯¹
        - timeframe: æ—¶é—´å‘¨æœŸ
        - limit: æ•°é‡é™åˆ¶ï¼ˆç›®æ ‡Kçº¿æ•°é‡ï¼Œé»˜è®¤1000ï¼‰
        - force_fetch: å¼ºåˆ¶æ‹‰å–æœ€æ–°æ•°æ®ï¼ˆ00ç§’æ‰«ææ—¶ä½¿ç”¨ï¼‰
        
        è¿”å›:
        - (Kçº¿æ•°æ®, is_stale) å…ƒç»„
          - Kçº¿æ•°æ®: [[ts, o, h, l, c, v], ...]
          - is_stale: æ˜¯å¦ä¸ºé™ˆæ—§æ•°æ®ï¼ˆäº¤æ˜“æ‰€æœªæ›´æ–°ï¼‰
        """
        timeframe = timeframe or self.timeframe
        limit = limit or OHLCV_TARGET_BARS  # é»˜è®¤ç›®æ ‡1000æ ¹
        key = (symbol, timeframe)
        now_ms = int(time.time() * 1000)
        
        # è·å–æ—¶é—´å‘¨æœŸæ¯«ç§’æ•°
        tf_ms = _get_timeframe_ms(timeframe)
        safety_ms = 1500  # å®‰å…¨è¾¹é™… 1.5 ç§’
        
        # æ£€æŸ¥ç†”æ–­
        if self.is_circuit_broken("ohlcv", symbol):
            if key in self.ohlcv_cache:
                entry = self.ohlcv_cache[key]
                logger.info(f"[md-circuit] {symbol} {timeframe} ç†”æ–­ä¸­ï¼Œä½¿ç”¨ç¼“å­˜")
                self.metrics["cache_hits"] += 1
                return entry.data, True  # ç†”æ–­æ—¶æ ‡è®°ä¸º stale
            raise Exception(f"[ç†”æ–­ä¸­] æ— ç¼“å­˜Kçº¿æ•°æ®: {symbol} {timeframe}")
        
        # å•èˆªç­å»é‡é”
        with self.locks[key]:
            # ========== æ£€æŸ¥æ˜¯å¦éœ€è¦å…¨é‡åˆå§‹åŒ– ==========
            need_full_init = False
            if key not in self.ohlcv_cache:
                need_full_init = True
            elif not self.ohlcv_cache[key].is_initialized:
                # ç¼“å­˜å­˜åœ¨ä½†æœªå®Œæˆåˆå§‹åŒ–ï¼ˆå¯èƒ½ä¹‹å‰æ‹‰å–å¤±è´¥ï¼‰
                need_full_init = True
            elif self.ohlcv_cache[key].bars_count < 200:
                # Kçº¿æ•°é‡ä¸¥é‡ä¸è¶³ï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–
                need_full_init = True
                logger.debug(f"[md-reinit] {symbol} {timeframe} Kçº¿ä¸è¶³ï¼Œé‡æ–°åˆå§‹åŒ–")
            
            # ========== å…¨é‡åˆ†é¡µæ‹‰å–ï¼ˆé¦–æ¬¡åˆå§‹åŒ–ï¼‰==========
            if need_full_init:
                try:
                    data = self._fetch_full_history(symbol, timeframe, limit)
                    
                    # ğŸ”¥ ä¿®å¤ï¼šæ¥å—è¾ƒå°‘çš„Kçº¿æ•°æ®ï¼ˆæ–°ä¸Šçº¿å¸ç§å¯èƒ½æ•°æ®ä¸è¶³ï¼‰
                    # æœ€ä½è¦æ±‚ï¼šè‡³å°‘ 50 æ ¹ K çº¿æ‰èƒ½è¿›è¡ŒåŸºæœ¬çš„æŠ€æœ¯åˆ†æ
                    MIN_BARS_REQUIRED = 50
                    
                    if data and len(data) >= MIN_BARS_REQUIRED:
                        max_ts = max(candle[0] for candle in data)
                        
                        # åˆ›å»ºç¼“å­˜æ¡ç›®
                        self.ohlcv_cache[key] = OHLCVCacheEntry(
                            data=data,
                            last_max_ts=max_ts,
                            fetched_at_ms=now_ms,
                            is_stale=False,
                            stale_count=0,
                            bars_count=len(data),
                            is_initialized=True
                        )
                        
                        # ä»å¾…åˆå§‹åŒ–é˜Ÿåˆ—ç§»é™¤
                        if key in self.pending_init:
                            del self.pending_init[key]
                        
                        # ğŸ”¥ å¦‚æœæ•°æ®ä¸è¶³ç›®æ ‡æ•°é‡ï¼Œæ‰“å°è­¦å‘Šä½†ä¸å¤±è´¥
                        if len(data) < limit:
                            logger.debug(f"[md-init] {symbol} {timeframe} æ•°æ®ä¸è¶³ç›®æ ‡ ({len(data)}/{limit} bars)")
                        else:
                            logger.debug(f"[md-init] {symbol} {timeframe} å…¨é‡æ‹‰å–å®Œæˆ {len(data)} bars")
                        
                        self.reset_circuit_breaker("ohlcv", symbol)
                        return data, False
                    elif data and len(data) > 0:
                        # ğŸ”¥ æ•°æ®å¤ªå°‘ï¼ˆ< 50 æ ¹ï¼‰ï¼Œè®°å½•è­¦å‘Šä½†ä»ç„¶ç¼“å­˜
                        max_ts = max(candle[0] for candle in data)
                        self.ohlcv_cache[key] = OHLCVCacheEntry(
                            data=data,
                            last_max_ts=max_ts,
                            fetched_at_ms=now_ms,
                            is_stale=False,
                            stale_count=0,
                            bars_count=len(data),
                            is_initialized=True  # æ ‡è®°ä¸ºå·²åˆå§‹åŒ–ï¼Œé¿å…é‡å¤æ‹‰å–
                        )
                        logger.debug(f"[md-init] {symbol} {timeframe} Kçº¿æ•°é‡è¿‡å°‘ ({len(data)} bars)")
                        return data, False
                    else:
                        raise Exception(f"å…¨é‡æ‹‰å–è¿”å›ç©ºæ•°æ®: {symbol} {timeframe}")
                        
                except Exception as e:
                    # è®°å½•åˆ°å¾…åˆå§‹åŒ–é˜Ÿåˆ—
                    retry_count = self.pending_init.get(key, 0) + 1
                    self.pending_init[key] = retry_count
                    logger.error(f"[md-init-fail] {symbol} {timeframe} å…¨é‡æ‹‰å–å¤±è´¥ (é‡è¯•æ¬¡æ•°: {retry_count}): {e}")
                    self.update_circuit_breaker("ohlcv", symbol)
                    raise
            
            # ========== å¢é‡æ›´æ–°ï¼ˆå·²åˆå§‹åŒ–çš„ç¼“å­˜ï¼‰==========
            entry = self.ohlcv_cache[key]
            
            # è®¡ç®—æ˜¯å¦åº”è¯¥æœ‰æ–° Kçº¿
            expected_new_ts = entry.last_max_ts + tf_ms
            
            # ğŸ”¥ force_fetch=True æ—¶è·³è¿‡ç¼“å­˜æ–°é²œåº¦æ£€æŸ¥ï¼Œå¼ºåˆ¶æ‹‰å–
            if not force_fetch and now_ms < expected_new_ts + safety_ms:
                # æœªåˆ°æ–° Kçº¿æ—¶é—´ï¼Œç›´æ¥è¿”å›ç¼“å­˜ï¼ˆä»æ–°é²œï¼‰
                self.metrics["cache_hits"] += 1
                logger.debug(f"[md-fresh] {symbol} {timeframe} ç¼“å­˜æ–°é²œï¼Œè·æ–°Kçº¿ {(expected_new_ts - now_ms)/1000:.1f}s")
                return entry.data, False
            
            # ğŸ”¥ æ‰§è¡Œå¢é‡æ‹‰å–ï¼ˆåªæ‹‰å–æœ€æ–°çš„å‡ åæ ¹ï¼‰
            try:
                new_data = self._fetch_incremental(symbol, timeframe, entry.last_max_ts)
                
                self.metrics["cache_misses"] += 1
                
                if new_data and len(new_data) > 0:
                    new_max_ts = max(candle[0] for candle in new_data)
                    
                    if new_max_ts > entry.last_max_ts:
                        # ğŸ”¥ æœ‰æ–° Kçº¿ï¼Œåˆå¹¶æ•°æ®å¹¶ä¿æŒå›ºå®šé•¿åº¦
                        bars_added = sum(1 for c in new_data if c[0] > entry.last_max_ts)
                        merged_data = self._merge_ohlcv(entry.data, new_data, limit)
                        
                        # æ›´æ–°ç¼“å­˜
                        self.ohlcv_cache[key] = OHLCVCacheEntry(
                            data=merged_data,
                            last_max_ts=new_max_ts,
                            fetched_at_ms=now_ms,
                            is_stale=False,
                            stale_count=0,
                            bars_count=len(merged_data),
                            is_initialized=True
                        )
                        
                        logger.debug(f"[md-incr] {symbol} {timeframe} +{bars_added} bars, total={len(merged_data)}")
                        self.reset_circuit_breaker("ohlcv", symbol)
                        return merged_data, False
                    else:
                        # ğŸ”¥ äº¤æ˜“æ‰€è¿˜æ²¡æ›´æ–°ï¼Œæ ‡è®°ä¸º stale
                        entry.stale_count += 1
                        entry.is_stale = True
                        entry.fetched_at_ms = now_ms
                        
                        if entry.stale_count >= 3:
                            logger.warning(f"[md-warn] {symbol} {timeframe} stale_count={entry.stale_count} (äº¤æ˜“æ‰€å»¶è¿Ÿ)")
                        else:
                            logger.debug(f"[md-stale] {symbol} {timeframe} stale_count={entry.stale_count}")
                        
                        return entry.data, True
                else:
                    # å¢é‡æ‹‰å–è¿”å›ç©ºæ•°æ®
                    entry.stale_count += 1
                    entry.is_stale = True
                    logger.debug(f"[md-stale] {symbol} {timeframe} å¢é‡æ‹‰å–è¿”å›ç©ºæ•°æ®")
                    return entry.data, True
                    
            except Exception as e:
                # å¢é‡æ‹‰å–å¤±è´¥ï¼Œè¿”å›æ—§ç¼“å­˜
                logger.warning(f"[md-error] {symbol} {timeframe} å¢é‡æ‹‰å–å¤±è´¥: {e}ï¼Œä½¿ç”¨æ—§ç¼“å­˜")
                self.update_circuit_breaker("ohlcv", symbol)
                return entry.data, True
    
    def _fetch_full_history(self, symbol: str, timeframe: str, target_bars: int) -> list:
        """
        ğŸ”¥ åˆ†é¡µå¾ªç¯æ‹‰å–å…¨é‡å†å²Kçº¿ï¼ˆå€’åºç­–ç•¥ï¼‰
        
        OKX å•æ¬¡åªè¿”å› 100/300 æ ¹ï¼Œéœ€è¦å¤šæ¬¡è¯·æ±‚æ‹¼æ¥ç›´åˆ°å‡‘å¤Ÿç›®æ ‡æ•°é‡
        
        ç­–ç•¥ï¼šä»æœ€æ–°æ•°æ®å‘è¿‡å»æ‹‰å–ï¼ˆå€’åºåˆ†é¡µï¼‰
        - ç¬¬ä¸€æ¬¡ä¸å¸¦ sinceï¼Œè·å–æœ€æ–°çš„ 100 æ ¹
        - åç»­ä½¿ç”¨ since = æœ€å°æ—¶é—´æˆ³ - 1ï¼Œå‘è¿‡å»æ‹‰å–
        - ç›´åˆ°å‡‘å¤Ÿç›®æ ‡æ•°é‡æˆ–æ— æ›´å¤šæ•°æ®
        
        å‚æ•°:
        - symbol: äº¤æ˜“å¯¹
        - timeframe: æ—¶é—´å‘¨æœŸ
        - target_bars: ç›®æ ‡Kçº¿æ•°é‡
        
        è¿”å›:
        - Kçº¿æ•°æ®åˆ—è¡¨ [[ts, o, h, l, c, v], ...]ï¼ˆæŒ‰æ—¶é—´å‡åºï¼‰
        """
        tf_ms = _get_timeframe_ms(timeframe)
        all_candles = []
        seen_timestamps = set()
        
        page_count = 0
        # ç¬¬ä¸€æ¬¡ä¸å¸¦ sinceï¼Œè·å–æœ€æ–°æ•°æ®
        current_end_ts = None
        
        logger.debug(f"[md-full] {symbol} {timeframe} å¼€å§‹å€’åºåˆ†é¡µæ‹‰å–ï¼Œç›®æ ‡ {target_bars} bars")
        
        while len(all_candles) < target_bars and page_count < OHLCV_MAX_PAGES:
            page_count += 1
            
            try:
                # æ„å»ºè¯·æ±‚å‚æ•°
                if current_end_ts is None:
                    # ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼šä¸å¸¦ sinceï¼Œè·å–æœ€æ–°æ•°æ®
                    data, _ = self._request_with_retry(
                        "ohlcv", symbol,
                        lambda: self.exchange.fetch_ohlcv(
                            symbol=symbol,
                            timeframe=timeframe,
                            limit=OHLCV_PAGE_SIZE
                        )
                    )
                else:
                    # åç»­è¯·æ±‚ï¼šä½¿ç”¨ params.after å‘è¿‡å»æ‹‰å–ï¼ˆOKX ç‰¹æœ‰å‚æ•°ï¼‰
                    # OKX çš„ after å‚æ•°è¡¨ç¤ºè·å–è¯¥æ—¶é—´æˆ³ä¹‹å‰çš„æ•°æ®
                    end_ts = current_end_ts
                    data, _ = self._request_with_retry(
                        "ohlcv", symbol,
                        lambda: self.exchange.fetch_ohlcv(
                            symbol=symbol,
                            timeframe=timeframe,
                            limit=OHLCV_PAGE_SIZE,
                            params={'after': str(end_ts)}
                        )
                    )
                
                if not data or len(data) == 0:
                    logger.debug(f"[md-full] {symbol} {timeframe} ç¬¬{page_count}é¡µè¿”å›ç©ºæ•°æ®ï¼Œåœæ­¢åˆ†é¡µ")
                    break
                
                # å»é‡å¹¶æ·»åŠ 
                new_count = 0
                min_ts_in_page = float('inf')
                for candle in data:
                    ts = candle[0]
                    if ts not in seen_timestamps:
                        seen_timestamps.add(ts)
                        all_candles.append(candle)
                        new_count += 1
                    if ts < min_ts_in_page:
                        min_ts_in_page = ts
                
                logger.debug(f"[md-full] {symbol} {timeframe} ç¬¬{page_count}é¡µ: +{new_count} bars, ç´¯è®¡ {len(all_candles)}")
                
                if new_count == 0:
                    # æ²¡æœ‰æ–°æ•°æ®ï¼Œå¯èƒ½å·²åˆ°è¾¾å†å²æœ€æ—©
                    logger.debug(f"[md-full] {symbol} {timeframe} æ— æ–°æ•°æ®ï¼Œåœæ­¢åˆ†é¡µ")
                    break
                
                # æ›´æ–° end_ts ä¸ºæœ¬é¡µæœ€å°æ—¶é—´æˆ³ï¼Œç”¨äºä¸‹ä¸€é¡µè¯·æ±‚
                current_end_ts = min_ts_in_page
                
                # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è§¦å‘é™æµ
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"[md-full] {symbol} {timeframe} ç¬¬{page_count}é¡µæ‹‰å–å¤±è´¥: {e}")
                break
        
        # æŒ‰æ—¶é—´æˆ³æ’åºï¼ˆå‡åºï¼‰
        all_candles.sort(key=lambda x: x[0])
        
        # ä¿ç•™æœ€æ–°çš„ target_bars æ ¹
        if len(all_candles) > target_bars:
            all_candles = all_candles[-target_bars:]
        
        logger.debug(f"[md-full] {symbol} {timeframe} åˆ†é¡µæ‹‰å–å®Œæˆ: {page_count} é¡µ, {len(all_candles)} bars")
        
        return all_candles
    
    def _fetch_incremental(self, symbol: str, timeframe: str, since_ts: int) -> list:
        """
        ğŸ”¥ å¢é‡æ‹‰å–æœ€æ–°Kçº¿
        
        åªè¯·æ±‚ since_ts ä¹‹åçš„æ•°æ®ï¼Œæ•°é‡é™åˆ¶ä¸º OHLCV_INCREMENTAL_LIMIT
        
        å‚æ•°:
        - symbol: äº¤æ˜“å¯¹
        - timeframe: æ—¶é—´å‘¨æœŸ
        - since_ts: èµ·å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        
        è¿”å›:
        - æ–°Kçº¿æ•°æ®åˆ—è¡¨
        """
        try:
            data, _ = self._request_with_retry(
                "ohlcv", symbol,
                lambda: self.exchange.fetch_ohlcv(
                    symbol=symbol,
                    timeframe=timeframe,
                    since=since_ts,
                    limit=OHLCV_INCREMENTAL_LIMIT
                )
            )
            return data if data else []
        except Exception as e:
            logger.warning(f"[md-incr] {symbol} {timeframe} å¢é‡æ‹‰å–å¤±è´¥: {e}")
            raise
    
    def _merge_ohlcv(self, cached_data: list, new_data: list, limit: int) -> list:
        """
        åˆå¹¶Kçº¿æ•°æ®ï¼Œå»é‡å¹¶ä¿ç•™æœ€æ–°çš„ limit æ ¹ï¼ˆå›ºå®šé•¿åº¦ç¼“å­˜ï¼‰
        
        å‚æ•°:
        - cached_data: ç¼“å­˜çš„Kçº¿æ•°æ®
        - new_data: æ–°æ‹‰å–çš„Kçº¿æ•°æ®
        - limit: ä¿ç•™æ•°é‡ï¼ˆé»˜è®¤1000ï¼‰
        
        è¿”å›:
        - åˆå¹¶åçš„Kçº¿æ•°æ®ï¼ˆå›ºå®šé•¿åº¦ï¼‰
        """
        # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸º key å»é‡
        ts_map = {}
        for candle in cached_data:
            ts_map[candle[0]] = candle
        for candle in new_data:
            ts_map[candle[0]] = candle  # æ–°æ•°æ®è¦†ç›–æ—§æ•°æ®
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        sorted_candles = sorted(ts_map.values(), key=lambda x: x[0])
        
        # ğŸ”¥ å…³é”®ï¼šæ‰§è¡Œ tail(limit) ä¿æŒç¼“å­˜é•¿åº¦æ’å®šï¼Œä¸¢å¼ƒè¿‡æœŸæ•°æ®
        if len(sorted_candles) > limit:
            sorted_candles = sorted_candles[-limit:]
        
        return sorted_candles
    
    def get_pending_init_symbols(self) -> list:
        """
        è·å–å¾…åˆå§‹åŒ–çš„å¸ç§åˆ—è¡¨
        
        è¿”å›:
        - [(symbol, timeframe, retry_count), ...]
        """
        return [(k[0], k[1], v) for k, v in self.pending_init.items()]
    
    def retry_pending_init(self, max_retries: int = 3) -> Dict[str, bool]:
        """
        é‡è¯•å¾…åˆå§‹åŒ–çš„å¸ç§
        
        å‚æ•°:
        - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¶…è¿‡åˆ™æ”¾å¼ƒ
        
        è¿”å›:
        - {(symbol, timeframe): success} å­—å…¸
        """
        results = {}
        keys_to_retry = list(self.pending_init.keys())
        
        for key in keys_to_retry:
            symbol, timeframe = key
            retry_count = self.pending_init.get(key, 0)
            
            if retry_count >= max_retries:
                logger.warning(f"[md-skip] {symbol} {timeframe} é‡è¯•æ¬¡æ•°è¶…é™ ({retry_count} >= {max_retries})ï¼Œè·³è¿‡")
                results[key] = False
                continue
            
            try:
                logger.info(f"[md-retry] {symbol} {timeframe} é‡è¯•åˆå§‹åŒ– (ç¬¬{retry_count + 1}æ¬¡)")
                self.get_ohlcv(symbol, timeframe, force_fetch=True)
                results[key] = True
            except Exception as e:
                logger.error(f"[md-retry-fail] {symbol} {timeframe} é‡è¯•å¤±è´¥: {e}")
                results[key] = False
        
        return results
    
    def get_cache_status(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜çŠ¶æ€æ‘˜è¦
        
        è¿”å›:
        - ç¼“å­˜çŠ¶æ€å­—å…¸
        """
        status = {
            "total_cached": len(self.ohlcv_cache),
            "initialized": 0,
            "pending_init": len(self.pending_init),
            "details": []
        }
        
        for key, entry in self.ohlcv_cache.items():
            symbol, timeframe = key
            if entry.is_initialized:
                status["initialized"] += 1
            
            status["details"].append({
                "symbol": symbol,
                "timeframe": timeframe,
                "bars": entry.bars_count,
                "initialized": entry.is_initialized,
                "stale": entry.is_stale,
                "stale_count": entry.stale_count
            })
        
        return status
    
    def get_ohlcv_data_only(self, symbol, timeframe=None, limit=None) -> list:
        """
        è·å–Kçº¿æ•°æ®ï¼ˆä»…æ•°æ®ï¼Œä¸è¿”å› is_staleï¼‰- å…¼å®¹æ—§æ¥å£
        
        å‚æ•°:
        - symbol: äº¤æ˜“å¯¹
        - timeframe: æ—¶é—´å‘¨æœŸ
        - limit: æ•°é‡é™åˆ¶
        
        è¿”å›:
        - Kçº¿æ•°æ®
        """
        data, _ = self.get_ohlcv(symbol, timeframe, limit)
        return data
    
    def get_ticker(self, symbol):
        """
        è·å–å®æ—¶è¡Œæƒ…ï¼Œæ”¯æŒTTLç¼“å­˜å’Œå•èˆªç­å»é‡
        
        å‚æ•°:
        - symbol: äº¤æ˜“å¯¹
        
        è¿”å›:
        - å®æ—¶è¡Œæƒ…æ•°æ®
        """
        key = symbol
        now = time.time()
        
        # æ£€æŸ¥ç†”æ–­
        if self.is_circuit_broken("ticker", symbol):
            if key in self.ticker_cache:
                logger.info(f"[ç†”æ–­ä¸­] ä½¿ç”¨ç¼“å­˜çš„è¡Œæƒ…æ•°æ®: {symbol}")
                self.metrics["cache_hits"] += 1
                return self.ticker_cache[key][0]
            raise Exception(f"[ç†”æ–­ä¸­] æ— ç¼“å­˜è¡Œæƒ…æ•°æ®: {symbol}")
        
        # æ£€æŸ¥ç¼“å­˜
        if key in self.ticker_cache:
            cached_data, fetched_at, last_error = self.ticker_cache[key]
            if now - fetched_at < self.TICKER_TTL_SEC:
                self.metrics["cache_hits"] += 1
                return cached_data
        
        # å•èˆªç­å»é‡
        with self.locks[key]:
            if self.pending[key].is_set():
                self.pending[key].wait()
                if key in self.ticker_cache:
                    self.metrics["cache_hits"] += 1
                    return self.ticker_cache[key][0]
                raise Exception(f"ç­‰å¾…APIè°ƒç”¨è¶…æ—¶: {key}")
            
            self.pending[key].clear()
            
            try:
                # ä½¿ç”¨å¸¦é‡è¯•çš„è¯·æ±‚
                data, api_latency = self._request_with_retry(
                    "ticker", symbol, 
                    self.exchange.fetch_ticker, symbol
                )
                
                self.metrics["cache_misses"] += 1
                
                # æ›´æ–°ç¼“å­˜
                self.ticker_cache[key] = (data, time.time(), None)
                
                # é‡ç½®ç†”æ–­çŠ¶æ€
                self.reset_circuit_breaker("ticker", symbol)
                
                return data
            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ—§ç¼“å­˜å¯ä»¥è¿”å›
                if key in self.ticker_cache:
                    logger.warning(f"[è¡Œæƒ…è·å–å¤±è´¥] ä½¿ç”¨æ—§ç¼“å­˜: {symbol} - {e}")
                    self.metrics["cache_hits"] += 1
                    return self.ticker_cache[key][0]
                
                # æ›´æ–°ç†”æ–­çŠ¶æ€
                self.update_circuit_breaker("ticker", symbol)
                
                raise
            finally:
                self.pending[key].set()
    
    def get_balance(self, params=None):
        """
        è·å–è´¦æˆ·ä½™é¢ï¼Œæ”¯æŒTTLç¼“å­˜å’Œå•èˆªç­å»é‡
        
        å‚æ•°:
        - params: å¯é€‰å‚æ•°
        
        è¿”å›:
        - è´¦æˆ·ä½™é¢æ•°æ®
        """
        key = "balance"
        now = time.time()
        
        # æ£€æŸ¥ç†”æ–­
        if self.is_circuit_broken("balance", "global"):
            if key in self.balance_cache:
                logger.info(f"[ç†”æ–­ä¸­] ä½¿ç”¨ç¼“å­˜çš„ä½™é¢æ•°æ®")
                self.metrics["cache_hits"] += 1
                return self.balance_cache[key][0]
            raise Exception(f"[ç†”æ–­ä¸­] æ— ç¼“å­˜ä½™é¢æ•°æ®")
        
        # æ£€æŸ¥ç¼“å­˜
        if key in self.balance_cache:
            cached_data, fetched_at, last_error = self.balance_cache[key]
            if now - fetched_at < self.BALANCE_TTL_SEC:
                self.metrics["cache_hits"] += 1
                return cached_data
        
        # å•èˆªç­å»é‡
        with self.locks[key]:
            if self.pending[key].is_set():
                self.pending[key].wait()
                if key in self.balance_cache:
                    self.metrics["cache_hits"] += 1
                    return self.balance_cache[key][0]
                raise Exception(f"ç­‰å¾…APIè°ƒç”¨è¶…æ—¶: {key}")
            
            self.pending[key].clear()
            
            try:
                # ä½¿ç”¨å¸¦é‡è¯•çš„è¯·æ±‚
                data, api_latency = self._request_with_retry(
                    "balance", "global", 
                    self.exchange.fetch_balance, params
                )
                
                self.metrics["cache_misses"] += 1
                
                # æ›´æ–°ç¼“å­˜
                self.balance_cache[key] = (data, time.time(), None)
                
                # é‡ç½®ç†”æ–­çŠ¶æ€
                self.reset_circuit_breaker("balance", "global")
                
                return data
            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ—§ç¼“å­˜å¯ä»¥è¿”å›
                if key in self.balance_cache:
                    logger.warning(f"[ä½™é¢è·å–å¤±è´¥] ä½¿ç”¨æ—§ç¼“å­˜ - {e}")
                    self.metrics["cache_hits"] += 1
                    return self.balance_cache[key][0]
                
                # æ›´æ–°ç†”æ–­çŠ¶æ€
                self.update_circuit_breaker("balance", "global")
                
                raise
            finally:
                self.pending[key].set()
    
    def get_positions(self, symbols=None):
        """
        è·å–æŒä»“ä¿¡æ¯ï¼Œæ”¯æŒTTLç¼“å­˜å’Œå•èˆªç­å»é‡
        
        å‚æ•°:
        - symbols: äº¤æ˜“å¯¹åˆ—è¡¨
        
        è¿”å›:
        - æŒä»“æ•°æ®
        """
        key = "positions"
        now = time.time()
        
        # æ£€æŸ¥ç†”æ–­
        if self.is_circuit_broken("positions", "global"):
            if key in self.positions_cache:
                logger.info(f"[ç†”æ–­ä¸­] ä½¿ç”¨ç¼“å­˜çš„æŒä»“æ•°æ®")
                self.metrics["cache_hits"] += 1
                return self.positions_cache[key][0]
            raise Exception(f"[ç†”æ–­ä¸­] æ— ç¼“å­˜æŒä»“æ•°æ®")
        
        # æ£€æŸ¥ç¼“å­˜
        if key in self.positions_cache:
            cached_data, fetched_at, last_error = self.positions_cache[key]
            if now - fetched_at < self.POSITIONS_TTL_SEC:
                self.metrics["cache_hits"] += 1
                return cached_data
        
        # å•èˆªç­å»é‡
        with self.locks[key]:
            if self.pending[key].is_set():
                self.pending[key].wait()
                if key in self.positions_cache:
                    self.metrics["cache_hits"] += 1
                    return self.positions_cache[key][0]
                raise Exception(f"ç­‰å¾…APIè°ƒç”¨è¶…æ—¶: {key}")
            
            self.pending[key].clear()
            
            try:
                # ä½¿ç”¨å¸¦é‡è¯•çš„è¯·æ±‚
                data, api_latency = self._request_with_retry(
                    "positions", "global", 
                    self.exchange.fetch_positions, symbols
                )
                
                self.metrics["cache_misses"] += 1
                
                # æ›´æ–°ç¼“å­˜
                self.positions_cache[key] = (data, time.time(), None)
                
                # é‡ç½®ç†”æ–­çŠ¶æ€
                self.reset_circuit_breaker("positions", "global")
                
                return data
            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ—§ç¼“å­˜å¯ä»¥è¿”å›
                if key in self.positions_cache:
                    logger.warning(f"[æŒä»“è·å–å¤±è´¥] ä½¿ç”¨æ—§ç¼“å­˜ - {e}")
                    self.metrics["cache_hits"] += 1
                    return self.positions_cache[key][0]
                
                # æ›´æ–°ç†”æ–­çŠ¶æ€
                self.update_circuit_breaker("positions", "global")
                
                raise
            finally:
                self.pending[key].set()
    
    def invalidate_balance(self):
        """
        ä½¿ä½™é¢ç¼“å­˜å¤±æ•ˆ
        """
        if "balance" in self.balance_cache:
            del self.balance_cache["balance"]
            logger.info("ä½™é¢ç¼“å­˜å·²å¤±æ•ˆ")
    
    def invalidate_positions(self):
        """
        ä½¿æŒä»“ç¼“å­˜å¤±æ•ˆ
        """
        if "positions" in self.positions_cache:
            del self.positions_cache["positions"]
            logger.info("æŒä»“ç¼“å­˜å·²å¤±æ•ˆ")
    
    def invalidate_ohlcv(self, symbol, timeframe=None, limit=None):
        """
        ä½¿Kçº¿ç¼“å­˜å¤±æ•ˆ
        
        å‚æ•°:
        - symbol: äº¤æ˜“å¯¹
        - timeframe: æ—¶é—´å‘¨æœŸï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™æ¸…é™¤è¯¥ symbol çš„æ‰€æœ‰å‘¨æœŸç¼“å­˜ï¼‰
        - limit: æ•°é‡é™åˆ¶ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
        """
        if timeframe:
            key = (symbol, timeframe)
            if key in self.ohlcv_cache:
                del self.ohlcv_cache[key]
                logger.debug(f"[md-invalidate] {symbol} {timeframe} ç¼“å­˜å·²æ¸…é™¤")
        else:
            # æ¸…é™¤è¯¥ symbol çš„æ‰€æœ‰å‘¨æœŸç¼“å­˜
            keys_to_delete = [k for k in self.ohlcv_cache.keys() if k[0] == symbol]
            for key in keys_to_delete:
                del self.ohlcv_cache[key]
            if keys_to_delete:
                logger.debug(f"[md-invalidate] {symbol} æ‰€æœ‰å‘¨æœŸç¼“å­˜å·²æ¸…é™¤ ({len(keys_to_delete)} ä¸ª)")
    
    def invalidate_ticker(self, symbol):
        """
        ä½¿è¡Œæƒ…ç¼“å­˜å¤±æ•ˆ
        
        å‚æ•°:
        - symbol: äº¤æ˜“å¯¹
        """
        if symbol in self.ticker_cache:
            del self.ticker_cache[symbol]
            logger.info(f"è¡Œæƒ…ç¼“å­˜å·²å¤±æ•ˆ: {symbol}")
    
    def is_circuit_broken(self, endpoint, symbol):
        """
        æ£€æŸ¥ç‰¹å®šç«¯ç‚¹å’Œäº¤æ˜“å¯¹çš„ç†”æ–­çŠ¶æ€
        
        å‚æ•°:
        - endpoint: ç«¯ç‚¹åç§°
        - symbol: äº¤æ˜“å¯¹
        
        è¿”å›:
        - æ˜¯å¦ç†”æ–­
        """
        now = time.time()
        key = f"{endpoint}:{symbol}"
        
        if key not in self.circuit_breakers:
            return False
        
        state = self.circuit_breakers[key]
        if now < state["until"]:
            return True
        
        # ç†”æ–­å·²è¿‡æœŸ
        del self.circuit_breakers[key]
        return False
    
    def update_circuit_breaker(self, endpoint, symbol):
        """
        æ›´æ–°ç†”æ–­çŠ¶æ€
        
        å‚æ•°:
        - endpoint: ç«¯ç‚¹åç§°
        - symbol: äº¤æ˜“å¯¹
        """
        key = f"{endpoint}:{symbol}"
        now = time.time()
        
        if key not in self.circuit_breakers:
            self.circuit_breakers[key] = {
                "failures": 1,
                "until": 0
            }
        else:
            self.circuit_breakers[key]["failures"] += 1
        
        failures = self.circuit_breakers[key]["failures"]
        
        # è¿ç»­5æ¬¡å¤±è´¥è§¦å‘ç†”æ–­
        if failures >= 5:
            # ç†”æ–­æ—¶é—´30-60ç§’éšæœº
            cooldown = random.randint(30, 60)
            self.circuit_breakers[key]["until"] = now + cooldown
            logger.warning(f"[ç†”æ–­è§¦å‘] {endpoint} {symbol}: {failures}æ¬¡å¤±è´¥ â†’ ç†”æ–­{cooldown}ç§’")
    
    def reset_circuit_breaker(self, endpoint, symbol):
        """
        é‡ç½®ç†”æ–­çŠ¶æ€
        
        å‚æ•°:
        - endpoint: ç«¯ç‚¹åç§°
        - symbol: äº¤æ˜“å¯¹
        """
        key = f"{endpoint}:{symbol}"
        if key in self.circuit_breakers:
            del self.circuit_breakers[key]
    
    def record_error(self, endpoint, symbol, error_msg):
        """
        è®°å½•é”™è¯¯å¹¶å®ç°é”™è¯¯èŠ‚æµ
        
        å‚æ•°:
        - endpoint: ç«¯ç‚¹åç§°
        - symbol: äº¤æ˜“å¯¹
        - error_msg: é”™è¯¯ä¿¡æ¯
        """
        now = time.time()
        self.error_counts[(endpoint, symbol)] += 1
        
        # é”™è¯¯èŠ‚æµï¼Œæ¯30ç§’æ±‡æ€»ä¸€æ¬¡
        if now - self.last_error_summary > 30:
            summary = []
            for (ep, sym), count in self.error_counts.items():
                if count > 0:
                    summary.append(f"{ep} {sym}: {count}æ¬¡")
            
            if summary:
                logger.error(f"[é”™è¯¯æ±‡æ€»] {', '.join(summary)}")
            
            # é‡ç½®è®¡æ•°
            self.error_counts.clear()
            self.last_error_summary = now
    
    def get_metrics(self):
        """
        è·å–æŒ‡æ ‡æ•°æ®
        
        è¿”å›:
        - æŒ‡æ ‡å­—å…¸
        """
        avg_latency = 0
        if self.metrics["api_calls"] > 0:
            avg_latency = sum(self.metrics["api_latency_ms"]) / self.metrics["api_calls"]
        
        cache_hit_rate = 0
        total_requests = self.metrics["cache_hits"] + self.metrics["cache_misses"]
        if total_requests > 0:
            cache_hit_rate = self.metrics["cache_hits"] / total_requests
        
        return {
            "api_calls": self.metrics["api_calls"],
            "avg_api_latency_ms": round(avg_latency, 2),
            "cache_hits": self.metrics["cache_hits"],
            "cache_misses": self.metrics["cache_misses"],
            "cache_hit_rate": round(cache_hit_rate, 4),
            "errors": self.metrics["errors"],
            "circuit_breakers": len(self.circuit_breakers)
        }
    
    def reset_metrics(self):
        """
        é‡ç½®æŒ‡æ ‡
        """
        self.metrics = {
            "api_calls": 0,
            "api_latency_ms": [],
            "cache_hits": 0,
            "cache_misses": 0,
            "errors": 0,
            "last_error_time": 0
        }
    
    # ============ ğŸ”¥ åŒé€šé“Kçº¿æ•°æ®æ”¯æŒ ============
    
    def get_dual_channel_ohlcv(
        self, 
        symbol: str, 
        timeframe: str = None, 
        limit: int = None,
        use_incremental: bool = True
    ) -> Tuple[Optional['DualChannelOHLCV'], bool]:
        """
        è·å–åŒé€šé“Kçº¿æ•°æ®
        
        æ˜ç¡®åŒºåˆ† forming_candle (candles[-1]) å’Œ last_closed_candle (candles[-2])
        
        å‚æ•°:
        - symbol: äº¤æ˜“å¯¹
        - timeframe: æ—¶é—´å‘¨æœŸ
        - limit: æ•°é‡é™åˆ¶
        - use_incremental: æ˜¯å¦ä½¿ç”¨å¢é‡æ‹‰å–ï¼ˆå·²åºŸå¼ƒï¼Œæ™ºèƒ½ç¼“å­˜è‡ªåŠ¨å¤„ç†ï¼‰
        
        è¿”å›:
        - (DualChannelOHLCV å¯¹è±¡, is_stale) å…ƒç»„
          - DualChannelOHLCV: å¦‚æœæ•°æ®ä¸è¶³åˆ™ä¸º None
          - is_stale: æ˜¯å¦ä¸ºé™ˆæ—§æ•°æ®
        """
        if not DUAL_CHANNEL_AVAILABLE:
            logger.warning("Dual channel OHLCV not available")
            return None, True
        
        timeframe = timeframe or self.timeframe
        limit = limit or self.ohlcv_limit
        
        # è·å–åŸå§‹Kçº¿æ•°æ®ï¼ˆä½¿ç”¨æ™ºèƒ½ç¼“å­˜ï¼‰
        candles, is_stale = self.get_ohlcv(symbol, timeframe, limit)
        
        if not candles or len(candles) < 2:
            logger.warning(f"Insufficient candles for dual channel: {symbol}/{timeframe}")
            return None, True
        
        try:
            # åˆ›å»º DualChannelOHLCV å¯¹è±¡
            dual_channel = DualChannelOHLCV.from_candles(
                symbol=symbol,
                timeframe=timeframe,
                candles=candles,
                fetch_time=int(time.time() * 1000)
            )
            
            return dual_channel, is_stale
            
        except InsufficientDataError as e:
            logger.warning(f"Insufficient data for dual channel: {e}")
            return None, True
        except Exception as e:
            logger.error(f"Error creating DualChannelOHLCV: {e}")
            return None, True
    
    def get_ohlcv_with_since(
        self, 
        symbol: str, 
        timeframe: str = None, 
        limit: int = None
    ) -> Tuple[list, bool]:
        """
        ä½¿ç”¨å¢é‡æ‹‰å–è·å–Kçº¿æ•°æ®ï¼ˆå·²æ•´åˆåˆ° get_ohlcvï¼Œæ­¤æ–¹æ³•ä¿ç•™å…¼å®¹æ€§ï¼‰
        
        å‚æ•°:
        - symbol: äº¤æ˜“å¯¹
        - timeframe: æ—¶é—´å‘¨æœŸ
        - limit: æ•°é‡é™åˆ¶
        
        è¿”å›:
        - (Kçº¿æ•°æ®, is_stale) å…ƒç»„
        """
        # ç›´æ¥è°ƒç”¨æ–°çš„æ™ºèƒ½ç¼“å­˜æ–¹æ³•
        return self.get_ohlcv(symbol, timeframe, limit)


# ============ ğŸ”¥ åŒ Key æœºåˆ¶ï¼šè¡Œæƒ…ä¸“ç”¨ Provider å·¥å‚ ============

def create_market_data_exchange(use_market_key: bool = True):
    """
    åˆ›å»ºè¡Œæƒ…æ•°æ®ä¸“ç”¨çš„äº¤æ˜“æ‰€é€‚é…å™¨
    
    ğŸ”¥ åŒ Key æœºåˆ¶ï¼š
    - use_market_key=True: ä¼˜å…ˆä½¿ç”¨è¡Œæƒ…ä¸“ç”¨ Key (MARKET_DATA_API_KEY)
    - use_market_key=False: ä½¿ç”¨äº¤æ˜“ Key (OKX_API_KEY)
    
    å‚æ•°:
    - use_market_key: æ˜¯å¦ä½¿ç”¨è¡Œæƒ…ä¸“ç”¨ Key
    
    è¿”å›:
    - (exchange_adapter, is_dedicated_key) å…ƒç»„
      - exchange_adapter: ccxt.okx å®ä¾‹
      - is_dedicated_key: æ˜¯å¦ä½¿ç”¨äº†ç‹¬ç«‹è¡Œæƒ… Key
    """
    import ccxt
    
    # ğŸ”¥ ä¼˜å…ˆä»æ•°æ®åº“è¯»å– Keyï¼ˆUI é…ç½®çš„ Keyï¼‰
    market_key = ""
    market_secret = ""
    market_passphrase = ""
    trade_key = ""
    trade_secret = ""
    trade_passphrase = ""
    
    try:
        from config_manager import get_config_manager
        config_mgr = get_config_manager()
        creds = config_mgr.load_credentials()  # ä¿®æ­£æ–¹æ³•å
        
        # ä»æ•°æ®åº“è¯»å–è¡Œæƒ…ä¸“ç”¨ Key
        if creds.has_market_key():
            market_key = creds.market_api_key
            market_secret = creds.market_api_secret
            market_passphrase = creds.market_api_passphrase
            logger.debug("[MarketData] ä»é…ç½®æ–‡ä»¶åŠ è½½è¡Œæƒ… Key")
        
        # ä»æ•°æ®åº“è¯»å–äº¤æ˜“ Key
        if creds.has_trade_key():
            trade_key = creds.trade_api_key
            trade_secret = creds.trade_api_secret
            trade_passphrase = creds.trade_api_passphrase
            logger.debug("[MarketData] ä»é…ç½®æ–‡ä»¶åŠ è½½äº¤æ˜“ Key")
    except Exception as e:
        logger.debug(f"[MarketData] é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°ç¯å¢ƒå˜é‡: {e}")
    
    # å›é€€åˆ°ç¯å¢ƒå˜é‡
    if not market_key:
        market_key = os.getenv("MARKET_DATA_API_KEY", "")
        market_secret = os.getenv("MARKET_DATA_SECRET", "")
        market_passphrase = os.getenv("MARKET_DATA_PASSPHRASE", "")
    
    if not trade_key:
        trade_key = os.getenv("OKX_API_KEY", "")
        trade_secret = os.getenv("OKX_API_SECRET", "")
        trade_passphrase = os.getenv("OKX_API_PASSPHRASE", "")
    
    # å†³å®šä½¿ç”¨å“ªå¥— Key
    is_dedicated_key = False
    if use_market_key and market_key and market_secret and market_passphrase:
        api_key = market_key
        api_secret = market_secret
        api_passphrase = market_passphrase
        is_dedicated_key = True
        logger.info("[MarketData] ä½¿ç”¨ç‹¬ç«‹è¡Œæƒ… Key ğŸ”")
    else:
        api_key = trade_key
        api_secret = trade_secret
        api_passphrase = trade_passphrase
        if use_market_key:
            logger.debug("[MarketData] æœªé…ç½®ç‹¬ç«‹è¡Œæƒ… Keyï¼Œä½¿ç”¨äº¤æ˜“ Key")
        else:
            logger.info("[MarketData] ä½¿ç”¨äº¤æ˜“ Key")
    
    # è·å–ä»£ç†é…ç½®
    http_proxy = os.getenv('HTTP_PROXY') or os.getenv('http_proxy')
    https_proxy = os.getenv('HTTPS_PROXY') or os.getenv('https_proxy')
    
    # åˆ›å»º ccxt é…ç½®
    config = {
        'enableRateLimit': True,
        'options': {
            'defaultType': 'swap',
        }
    }
    
    # æ·»åŠ ä»£ç†
    if https_proxy:
        config['proxies'] = {
            'http': http_proxy or https_proxy,
            'https': https_proxy
        }
    
    # æ·»åŠ  API å‡­è¯
    if api_key and api_secret and api_passphrase:
        config['apiKey'] = api_key
        config['secret'] = api_secret
        config['password'] = api_passphrase
    
    exchange = ccxt.okx(config)
    return exchange, is_dedicated_key


def create_market_data_provider_with_dedicated_key(
    timeframe: str = '1m',
    ohlcv_limit: int = 1000,
    **kwargs
) -> 'MarketDataProvider':
    """
    åˆ›å»ºä½¿ç”¨è¡Œæƒ…ä¸“ç”¨ Key çš„ MarketDataProvider
    
    ğŸ”¥ åŒ Key æœºåˆ¶ï¼šè‡ªåŠ¨ä½¿ç”¨è¡Œæƒ…ä¸“ç”¨ Keyï¼Œä¸äº¤æ˜“æ¥å£éš”ç¦»
    
    å‚æ•°:
    - timeframe: é»˜è®¤æ—¶é—´å‘¨æœŸ
    - ohlcv_limit: é»˜è®¤ Kçº¿æ•°é‡
    - **kwargs: å…¶ä»– MarketDataProvider å‚æ•°
    
    è¿”å›:
    - MarketDataProvider å®ä¾‹
    """
    exchange, is_dedicated = create_market_data_exchange(use_market_key=True)
    
    provider = MarketDataProvider(
        exchange_adapter=exchange,
        timeframe=timeframe,
        ohlcv_limit=ohlcv_limit,
        **kwargs
    )
    
    # è®°å½• Key ç±»å‹
    provider._is_dedicated_market_key = is_dedicated
    
    return provider


# ============ ğŸ”¥ WebSocket æ•°æ®æºæ”¯æŒ ============

# WebSocket å®¢æˆ·ç«¯å¯¼å…¥
try:
    from okx_websocket import (
        OKXWebSocketClient, 
        get_ws_client, 
        start_ws_client, 
        stop_ws_client,
        is_ws_available,
        WEBSOCKET_AVAILABLE
    )
    WS_IMPORT_OK = True
except ImportError:
    WS_IMPORT_OK = False
    WEBSOCKET_AVAILABLE = False
    logger.warning("okx_websocket module not available")


class WebSocketMarketDataProvider:
    """
    WebSocket æ•°æ®æºæä¾›è€…
    
    ç‰¹ç‚¹ï¼š
    - å®æ—¶æ¨é€ï¼Œä½å»¶è¿Ÿ
    - è‡ªåŠ¨é‡è¿
    - ä¸ REST æ•°æ®æºå¯åˆ‡æ¢
    
    ä½¿ç”¨åœºæ™¯ï¼š
    - Kçº¿å›¾å®æ—¶æ›´æ–°ï¼ˆå›ºå®šä½¿ç”¨ WebSocketï¼‰
    - äº¤æ˜“å¼•æ“å¯é€‰æ•°æ®æº
    """
    
    def __init__(self, use_aws: bool = False, fallback_provider: MarketDataProvider = None):
        """
        åˆå§‹åŒ– WebSocket æ•°æ®æº
        
        Args:
            use_aws: æ˜¯å¦ä½¿ç”¨ AWS èŠ‚ç‚¹
            fallback_provider: REST å›é€€æ•°æ®æº
        """
        self.use_aws = use_aws
        self.fallback_provider = fallback_provider
        self.ws_client: Optional[OKXWebSocketClient] = None
        self._subscribed_symbols: Dict[str, str] = {}  # {symbol: timeframe}
        
        # ğŸ”¥ æœ¬åœ°å†å²æ•°æ®ç¼“å­˜ï¼ˆæ··åˆæ¨¡å¼æ ¸å¿ƒï¼‰
        # {symbol: {timeframe: {'data': [...], 'last_ts': int, 'initialized': bool}}}
        self._history_cache: Dict[str, Dict[str, Dict]] = {}
        self._cache_lock = threading.Lock()
        
        # åˆå§‹åŒ– WebSocket å®¢æˆ·ç«¯
        if WS_IMPORT_OK and WEBSOCKET_AVAILABLE:
            self.ws_client = get_ws_client(use_aws)
        else:
            logger.warning("[WS-Provider] WebSocket ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ REST å›é€€")
    
    def start(self) -> bool:
        """å¯åŠ¨ WebSocket è¿æ¥"""
        if self.ws_client:
            return self.ws_client.start()
        return False
    
    def stop(self):
        """åœæ­¢ WebSocket è¿æ¥"""
        if self.ws_client:
            self.ws_client.stop()
    
    def is_connected(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²è¿æ¥"""
        return self.ws_client and self.ws_client.is_connected()
    
    def subscribe(self, symbol: str, timeframe: str = "1m") -> bool:
        """
        è®¢é˜… Kçº¿æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
        
        Returns:
            æ˜¯å¦è®¢é˜…æˆåŠŸ
        """
        if not self.ws_client:
            return False
        
        # ç¡®ä¿è¿æ¥
        if not self.ws_client.is_connected():
            if not self.ws_client.start():
                logger.warning(f"[WS-Provider] æ— æ³•è¿æ¥ï¼Œè®¢é˜…å¤±è´¥: {symbol}")
                return False
        
        # è®¢é˜… Kçº¿
        success = self.ws_client.subscribe_candles(symbol, timeframe)
        if success:
            self._subscribed_symbols[symbol] = timeframe
        
        return success
    
    def unsubscribe(self, symbol: str) -> bool:
        """å–æ¶ˆè®¢é˜…"""
        if not self.ws_client:
            return False
        
        timeframe = self._subscribed_symbols.get(symbol, "1m")
        success = self.ws_client.unsubscribe(symbol, "candle", timeframe)
        
        if symbol in self._subscribed_symbols:
            del self._subscribed_symbols[symbol]
        
        return success
    
    def get_ohlcv(
        self, 
        symbol: str, 
        timeframe: str = "1m", 
        limit: int = 500,
        fallback_to_rest: bool = True
    ) -> Tuple[list, bool]:
        """
        è·å– Kçº¿æ•°æ®ï¼ˆæ··åˆæ¨¡å¼ï¼‰
        
        ğŸ”¥ æ··åˆæ¨¡å¼é€»è¾‘ï¼š
        1. é¦–æ¬¡è¯·æ±‚ï¼šç”¨ REST æ‹‰å–å®Œæ•´å†å²æ•°æ®ï¼Œç¼“å­˜åˆ°æœ¬åœ°
        2. åç»­è¯·æ±‚ï¼šç”¨ WebSocket å¢é‡æ›´æ–°æœ€æ–° K çº¿
        
        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
            limit: æ•°é‡é™åˆ¶
            fallback_to_rest: æ˜¯å¦å›é€€åˆ° REST
        
        Returns:
            (Kçº¿æ•°æ®, is_from_ws) å…ƒç»„
        """
        cache_key = f"{symbol}:{timeframe}"
        
        with self._cache_lock:
            # åˆå§‹åŒ–ç¼“å­˜ç»“æ„
            if symbol not in self._history_cache:
                self._history_cache[symbol] = {}
            if timeframe not in self._history_cache[symbol]:
                self._history_cache[symbol][timeframe] = {
                    'data': [],
                    'last_ts': 0,
                    'initialized': False
                }
            
            cache_entry = self._history_cache[symbol][timeframe]
        
        # ğŸ”¥ é¦–æ¬¡è¯·æ±‚ï¼šç”¨ REST æ‹‰å–å®Œæ•´å†å²æ•°æ®
        if not cache_entry['initialized']:
            if fallback_to_rest and self.fallback_provider:
                logger.info(f"[WS-Provider] é¦–æ¬¡åŠ è½½ {symbol} {timeframe}ï¼Œä½¿ç”¨ REST æ‹‰å–å†å²æ•°æ®...")
                data, is_stale = self.fallback_provider.get_ohlcv(symbol, timeframe, limit)
                
                if data and len(data) > 0:
                    with self._cache_lock:
                        cache_entry['data'] = data
                        cache_entry['last_ts'] = data[-1][0] if data else 0
                        cache_entry['initialized'] = True
                    
                    # ç¡®ä¿ WebSocket å·²è®¢é˜…
                    if self.ws_client and self.ws_client.is_connected():
                        if symbol not in self._subscribed_symbols:
                            self.subscribe(symbol, timeframe)
                    
                    logger.info(f"[WS-Provider] {symbol} {timeframe} å†å²æ•°æ®å·²ç¼“å­˜: {len(data)} bars")
                    return data, False
                else:
                    return [], False
            else:
                return [], False
        
        # ğŸ”¥ åç»­è¯·æ±‚ï¼šç”¨ WebSocket å¢é‡æ›´æ–°
        if self.ws_client and self.ws_client.is_connected():
            # ç¡®ä¿å·²è®¢é˜…
            if symbol not in self._subscribed_symbols:
                self.subscribe(symbol, timeframe)
            
            # è·å– WebSocket æœ€æ–°æ•°æ®
            ws_data = self.ws_client.get_candles(symbol, timeframe, 10)  # åªå–æœ€æ–°å‡ æ ¹
            
            if ws_data and len(ws_data) > 0:
                with self._cache_lock:
                    cached_data = cache_entry['data']
                    last_cached_ts = cache_entry['last_ts']
                    
                    # åˆå¹¶æ–°æ•°æ®
                    updated = False
                    for candle in ws_data:
                        candle_ts = candle[0]
                        
                        if candle_ts > last_cached_ts:
                            # æ–° K çº¿ï¼Œè¿½åŠ 
                            cached_data.append(candle)
                            updated = True
                        elif candle_ts == last_cached_ts:
                            # æ›´æ–°æœ€åä¸€æ ¹ï¼ˆå¯èƒ½è¿˜åœ¨å½¢æˆä¸­ï¼‰
                            if cached_data:
                                cached_data[-1] = candle
                                updated = True
                    
                    if updated:
                        # ä¿æŒæ•°æ®é‡ä¸è¶…è¿‡ limit
                        if len(cached_data) > limit:
                            cached_data = cached_data[-limit:]
                        
                        cache_entry['data'] = cached_data
                        cache_entry['last_ts'] = cached_data[-1][0] if cached_data else 0
                    
                    result_data = cached_data[-limit:] if len(cached_data) > limit else cached_data
                
                logger.debug(f"[WS-Provider] {symbol} {timeframe} å¢é‡æ›´æ–°å®Œæˆ: {len(result_data)} bars")
                return result_data, True
        
        # WebSocket ä¸å¯ç”¨ï¼Œè¿”å›ç¼“å­˜æ•°æ®
        with self._cache_lock:
            cached_data = cache_entry['data']
            result_data = cached_data[-limit:] if len(cached_data) > limit else cached_data
        
        return result_data, False
    
    def get_ticker(self, symbol: str, fallback_to_rest: bool = True) -> Optional[Dict]:
        """
        è·å–å®æ—¶è¡Œæƒ…
        
        Args:
            symbol: äº¤æ˜“å¯¹
            fallback_to_rest: æ˜¯å¦å›é€€åˆ° REST
        
        Returns:
            è¡Œæƒ…æ•°æ®
        """
        # å°è¯•ä» WebSocket è·å–
        if self.ws_client and self.ws_client.is_connected():
            ticker = self.ws_client.get_ticker(symbol)
            if ticker:
                return ticker
        
        # å›é€€åˆ° REST
        if fallback_to_rest and self.fallback_provider:
            return self.fallback_provider.get_ticker(symbol)
        
        return None
    
    def get_last_price(self, symbol: str) -> Optional[float]:
        """è·å–æœ€æ–°ä»·æ ¼"""
        ticker = self.get_ticker(symbol)
        if ticker:
            return ticker.get("last")
        return None
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "ws_available": WS_IMPORT_OK and WEBSOCKET_AVAILABLE,
            "ws_connected": self.is_connected(),
            "subscribed_symbols": list(self._subscribed_symbols.keys()),
            "has_fallback": self.fallback_provider is not None
        }
        
        if self.ws_client:
            stats.update(self.ws_client.get_cache_stats())
        
        # æ·»åŠ æœ¬åœ°ç¼“å­˜ç»Ÿè®¡
        with self._cache_lock:
            cache_stats = {}
            for symbol, tf_data in self._history_cache.items():
                for tf, entry in tf_data.items():
                    key = f"{symbol}:{tf}"
                    cache_stats[key] = {
                        'bars': len(entry['data']),
                        'initialized': entry['initialized']
                    }
            stats['history_cache'] = cache_stats
        
        return stats
    
    def clear_cache(self, symbol: str = None, timeframe: str = None):
        """
        æ¸…é™¤æœ¬åœ°å†å²æ•°æ®ç¼“å­˜
        
        Args:
            symbol: æŒ‡å®šå¸ç§ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
            timeframe: æŒ‡å®šå‘¨æœŸï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
        """
        with self._cache_lock:
            if symbol is None:
                # æ¸…é™¤å…¨éƒ¨
                self._history_cache.clear()
                logger.info("[WS-Provider] å·²æ¸…é™¤å…¨éƒ¨å†å²æ•°æ®ç¼“å­˜")
            elif timeframe is None:
                # æ¸…é™¤æŒ‡å®šå¸ç§çš„å…¨éƒ¨å‘¨æœŸ
                if symbol in self._history_cache:
                    del self._history_cache[symbol]
                    logger.info(f"[WS-Provider] å·²æ¸…é™¤ {symbol} çš„å†å²æ•°æ®ç¼“å­˜")
            else:
                # æ¸…é™¤æŒ‡å®šå¸ç§çš„æŒ‡å®šå‘¨æœŸ
                if symbol in self._history_cache and timeframe in self._history_cache[symbol]:
                    del self._history_cache[symbol][timeframe]
                    logger.info(f"[WS-Provider] å·²æ¸…é™¤ {symbol} {timeframe} çš„å†å²æ•°æ®ç¼“å­˜")


def create_hybrid_market_data_provider(
    exchange_adapter,
    timeframe: str = '1m',
    ohlcv_limit: int = 1000,
    enable_websocket: bool = False,
    use_aws: bool = False,
    **kwargs
) -> Tuple[MarketDataProvider, Optional[WebSocketMarketDataProvider]]:
    """
    åˆ›å»ºæ··åˆæ•°æ®æºæä¾›è€…
    
    è¿”å› REST å’Œ WebSocket ä¸¤ä¸ªæä¾›è€…ï¼Œå¯æ ¹æ®é…ç½®åˆ‡æ¢
    
    Args:
        exchange_adapter: äº¤æ˜“æ‰€é€‚é…å™¨
        timeframe: é»˜è®¤æ—¶é—´å‘¨æœŸ
        ohlcv_limit: é»˜è®¤ Kçº¿æ•°é‡
        enable_websocket: æ˜¯å¦å¯ç”¨ WebSocket
        use_aws: WebSocket æ˜¯å¦ä½¿ç”¨ AWS èŠ‚ç‚¹
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        (rest_provider, ws_provider) å…ƒç»„
    """
    # åˆ›å»º REST æä¾›è€…
    rest_provider = MarketDataProvider(
        exchange_adapter=exchange_adapter,
        timeframe=timeframe,
        ohlcv_limit=ohlcv_limit,
        **kwargs
    )
    
    # åˆ›å»º WebSocket æä¾›è€…ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    ws_provider = None
    if enable_websocket and WS_IMPORT_OK and WEBSOCKET_AVAILABLE:
        ws_provider = WebSocketMarketDataProvider(
            use_aws=use_aws,
            fallback_provider=rest_provider
        )
        logger.info("[Hybrid] WebSocket æ•°æ®æºå·²åˆ›å»º")
    
    return rest_provider, ws_provider
